import pandas as pd
import numpy as np
from sklearn import preprocessing
from sklearn.model_selection import train_test_split
from sklearn import metrics
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
import matplotlib.pyplot as plt
%matplotlib inline
import seaborn as sns
data = pd.read_csv("data.csv")
data.head()
data.columns
data.shape
data.dtypes
pd.isnull(data).sum()
data.describe()
y = data['price_range']
x = data.drop('price_range', axis = 1)
y.unique()
labels = ["low cost", "medium cost", "high cost", "very high cost"]
values = data['price_range'].value_counts().values
colors = ['yellow','turquoise','lightblue', 'pink']
fig1, ax1 = plt.subplots()
ax1.pie(values, labels=labels, colors=colors, autopct='%1.1f%%', shadow=True, startangle=90)
ax1.set_title('balanced or imbalanced?')
plt.show()
x_train, x_valid, y_train, y_valid = train_test_split(x, y, test_size = 0.2, random_state = 101, stratify = y)
print(x_train.shape)
print(x_valid.shape)
fig = plt.subplots (figsize = (12, 12))
sns.heatmap(data.corr (), square = True, cbar = True, annot = True, cmap="GnBu", annot_kws = {'size': 8})
plt.title('Correlations between Attributes')
plt.show ()
lr = LogisticRegression(multi_class = 'multinomial', solver = 'sag',  max_iter = 10000)
lr.fit(x_train, y_train)
y_pred_lr = lr.predict(x_valid)
confusion_matrix = metrics.confusion_matrix(y_valid, y_pred_lr)
confusion_matrix
acc_lr = metrics.accuracy_score(y_valid, y_pred_lr)
acc_lr
dt = DecisionTreeClassifier(random_state=101)
dt_model = dt.fit(x_train, y_train)
y_pred_dt = dt.predict(x_valid)
dt_model
print(metrics.confusion_matrix(y_valid, y_pred_dt))
print(metrics.classification_report(y_valid, y_pred_dt))
acc_dt = metrics.accuracy_score(y_valid, y_pred_dt)
acc_dt
rf = RandomForestClassifier(n_estimators = 100, random_state=101, criterion = 'entropy', oob_score = True)
model_rf = rf.fit(x_train, y_train)
y_pred_rf = rf.predict(x_valid)
print(metrics.confusion_matrix(y_valid, y_pred_rf))
pd.crosstab(y_valid, y_pred_rf, rownames=['Actual Class'], colnames=['Predicted Class'])
acc_rf = metrics.accuracy_score(y_valid, y_pred_rf)
acc_rf
model_knn = KNeighborsClassifier(n_neighbors=3)
model_knn.fit(x_train, y_train)
y_pred_knn = model_knn.predict(x_valid)
print(metrics.confusion_matrix(y_valid, y_pred_knn))
print(accuracy_score(y_valid, y_pred_knn))
from sklearn.model_selection import GridSearchCV
parameters = {'n_neighbors':np.arange(1,30)}
knn = KNeighborsClassifier()

model = GridSearchCV(knn, parameters, cv=5)
model.fit(x_train, y_train)
model.best_params_
model_knn = KNeighborsClassifier(n_neighbors=9)
model_knn.fit(x_train, y_train)
y_pred_knn = model_knn.predict(x_valid)
print(metrics.confusion_matrix(y_valid, y_pred_knn))
acc_knn = accuracy_score(y_valid, y_pred_knn)
acc_knn
models = ['logistic regression', 'decision tree', 'random forest', 'knn']
acc_scores = [0.73, 0.83, 0.90, 0.95]

plt.bar(models, acc_scores, color=['lightblue', 'pink', 'lightgrey', 'cyan'])
plt.ylabel("accuracy scores")
plt.title("Which model is the most accurate?")
plt.show()
test_data = pd.read_csv("test_data.csv")
test_data.head()
test_data=test_data.drop('id',axis=1)
test_data.head()
predicted_price_range = model_knn.predict(test_data)
# We are able to forecast test dataset labels:
predicted_price_range

test_data['price_range'] = predicted_price_range
test_data.head()
